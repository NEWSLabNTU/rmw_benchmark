// Zenoh Configuration with Shared Memory for RMW Stress Tests
//
// This configuration enables shared memory transport for high-throughput
// image streaming benchmarks, optimized for 640x480@15fps to 4K@60fps tests.
//
// Key features:
// - Shared memory enabled with 1.4 GB pool (matches CycloneDDS/Iceoryx)
// - 1 KB threshold for SHM optimization (matches CycloneDDS, catches all image data)
// - QoS enabled for ROS 2 reliability/durability support
// - Optimized buffers (16 MB) for large message bursts
// - TCP transport on port 7448 (avoids conflict with planning sim)
//
// Based on: ros2/rmw_zenoh DEFAULT_RMW_ZENOH_SESSION_CONFIG.json5
// Customized for: High-bandwidth sensor streaming benchmarks

{
  /// Node mode: peer (connects to router, can communicate with other peers)
  mode: "peer",

  /// Connection endpoints
  connect: {
    /// Timeout configuration for different node types
    timeout_ms: { router: -1, peer: -1, client: 0 },

    /// Connect to Zenoh router on localhost
    /// Note: Router must be started separately (make start-zenoh-router)
    endpoints: [
      "tcp/localhost:7447"
    ],

    /// Exit on connection failure
    exit_on_failure: { router: false, peer: false, client: true },

    /// Connection retry configuration
    retry: {
      period_init_ms: 1000,
      period_max_ms: 4000,
      period_increase_factor: 2,
    },
  },

  /// Listen endpoints
  listen: {
    timeout_ms: 0,

    /// Listen on port 7448 (different from router's 7447)
    /// Accepts connections from localhost only (colocated nodes)
    endpoints: [
      "tcp/localhost:7448"
    ],

    exit_on_failure: true,

    retry: {
      period_init_ms: 1000,
      period_max_ms: 4000,
      period_increase_factor: 2,
    },
  },

  /// Session open behavior
  open: {
    return_conditions: {
      /// Wait for scouted peers/routers before returning
      connect_scouted: true,
      /// Wait for initial declares from connected peers
      declares: true,
    },
  },

  /// Discovery and scouting configuration
  scouting: {
    /// Client mode scouting timeout
    timeout: 3000,
    /// Peer mode scouting delay
    delay: 500,

    /// Multicast scouting: DISABLED (ROS recommendation)
    multicast: {
      enabled: false,
      address: "224.0.0.224:7446",
      interface: "auto",
      ttl: 1,
      autoconnect: { router: [], peer: ["router", "peer"], client: ["router"] },
      autoconnect_strategy: { peer: { to_router: "always", to_peer: "greater-zid" } },
      listen: true,
    },

    /// Gossip scouting: ENABLED
    gossip: {
      enabled: true,
      multihop: false,
      /// Peers send gossip only to router (reduces startup traffic)
      target: { router: ["router", "peer"], peer: ["router"]},
      autoconnect: { router: [], peer: ["router", "peer"] },
      autoconnect_strategy: { peer: { to_router: "always", to_peer: "greater-zid" } },
    },
  },

  /// Timestamping: ENABLED (required for transient_local durability)
  timestamping: {
    enabled: { router: true, peer: true, client: true },
    drop_future_timestamp: false,
  },

  /// Default query timeout: 10 minutes
  queries_default_timeout: 600000,

  /// Routing configuration
  routing: {
    router: {
      peers_failover_brokering: true,
    },
    peer: {
      /// Routing mode: peer_to_peer (simpler than linkstate)
      mode: "peer_to_peer",
    },
    interests: {
      timeout: 10000,
    },
  },

  /// Transport configuration (OPTIMIZED FOR HIGH THROUGHPUT)
  transport: {
    /// Unicast transport configuration
    unicast: {
      /// Increased timeouts for large number of nodes at startup
      open_timeout: 60000,
      accept_timeout: 60000,
      accept_pending: 10000,
      max_sessions: 10000,
      max_links: 1,

      /// Low latency mode: DISABLED (incompatible with QoS)
      lowlatency: false,

      /// QoS: ENABLED (required for ROS 2)
      qos: {
        enabled: true,
      },

      /// Compression: DISABLED (CPU overhead not worth it for local SHM)
      compression: {
        enabled: false,
      },
    },

    /// Multicast transport configuration
    multicast: {
      join_interval: 2500,
      max_sessions: 1000,
      qos: {
        enabled: false,
      },
      compression: {
        enabled: false,
      },
    },

    /// Link configuration
    link: {
      /// TX (transmit) configuration
      tx: {
        /// Sequence number resolution
        sequence_number_resolution: "32bit",

        /// Lease duration: 60 seconds (increased for ROS)
        lease: 60000,

        /// Keep-alive messages: 2 per lease period
        /// (Fewer needed for loopback communication)
        keep_alive: 2,

        /// Batch size: 64 KB (maximum)
        batch_size: 65535,

        /// TX queue configuration
        queue: {
          /// Queue sizes for each priority level
          size: {
            control: 2,
            real_time: 2,
            interactive_high: 2,
            interactive_low: 2,
            data_high: 2,
            data: 2,
            data_low: 2,
            background: 2,
          },

          /// Congestion control
          congestion_control: {
            drop: {
              /// Wait 1ms before dropping droppable messages
              wait_before_drop: 1000,
              max_wait_before_drop_fragments: 50000,
            },
            block: {
              /// Wait 60 seconds before closing transport (increased for ROS startup)
              wait_before_close: 60000000,
            },
          },

          /// Batching: ENABLED (adaptive, 1ms time limit)
          batching: {
            enabled: true,
            time_limit: 1,
          },

          /// Allocation mode: lazy (allocate on demand)
          allocation: {
            mode: "lazy",
          },
        },
      },

      /// RX (receive) configuration - INCREASED FOR HIGH THROUGHPUT
      rx: {
        /// Receiving buffer: 16 MB (increased from default 64 KB)
        /// Rationale: Handle burst of multiple large images in flight
        buffer_size: 16777216,

        /// Maximum defragmentation buffer: 1 GB (default)
        /// Sufficient for largest test message (4K image ~24 MB)
        max_message_size: 1073741824,
      },

      /// TLS configuration (unused, but kept for completeness)
      tls: {
        root_ca_certificate: null,
        listen_private_key: null,
        listen_certificate: null,
        enable_mtls: false,
        connect_private_key: null,
        connect_certificate: null,
        verify_name_on_connect: true,
        close_link_on_expiration: false,
      },
    },

    /// ========================================================================
    /// SHARED MEMORY CONFIGURATION (PRIMARY OPTIMIZATION)
    /// ========================================================================
    shared_memory: {
      /// ENABLED: Use shared memory for zero-copy message passing
      /// Required on both publisher and subscriber for SHM to activate
      enabled: true,

      /// Initialization mode: "init" (initialize at session open)
      /// - "init": SHM initialized immediately (no first-message latency)
      /// - "lazy": SHM initialized on first use (better startup, slight delay)
      /// Choice: "init" for consistent benchmark measurements
      mode: "init",

      /// Transport optimization for large messages
      transport_optimization: {
        /// ENABLED: Automatically use SHM for large messages
        enabled: true,

        /// Pool size: 1.4 GB (matches CycloneDDS/Iceoryx)
        ///
        /// RATIONALE:
        /// - Matches CycloneDDS/Iceoryx total pool size for fair comparison
        /// - CycloneDDS uses 5 tiered pools totaling 1,452,478,800 bytes
        /// - This config uses single unified pool of 1,500,000,000 bytes (1.4 GB)
        /// - Sufficient for all test loads from 640x480@15fps to 4K@60fps
        ///
        /// COMPARISON:
        /// - Default Zenoh: 16 MB (insufficient for HD+)
        /// - Previous 256 MB: Excellent up to 4K@30fps, 6.09% loss at 4K@45fps
        /// - Previous 512 MB: Excellent up to 4K@30fps, 1.64% loss at 4K@45fps
        /// - This config 1.4 GB: Should match CycloneDDS performance
        /// - CycloneDDS: 1.4 GB (tiered pools: 1KB, 128KB, 3MB, 9MB, 25MB)
        ///
        /// POOL STRUCTURE DIFFERENCE:
        /// - Zenoh: Single unified 1.4 GB pool (simple allocation)
        /// - CycloneDDS: 5 tiered pools optimized for different message sizes
        pool_size: 1500000000,

        /// Message size threshold: 1 KB
        ///
        /// RATIONALE:
        /// - Default is 3 KB, but we want to catch more messages
        /// - All image messages are well above 1 KB (smallest is 900 KB)
        /// - Lower threshold ensures all sensor data uses SHM
        /// - Small messages (< 1 KB) use regular transport (minimal overhead)
        ///
        /// COMPARISON:
        /// - Default Zenoh: 3 KB
        /// - This config: 1 KB (more aggressive SHM usage)
        /// - CycloneDDS: Variable per pool (1 KB to 25 MB)
        message_size_threshold: 1024,
      },
    },

    /// Authentication (unused)
    auth: {
      usrpwd: {
        user: null,
        password: null,
        dictionary_file: null,
      },
      pubkey: {
        public_key_pem: null,
        private_key_pem: null,
        public_key_file: null,
        private_key_file: null,
        key_size: null,
        known_keys_file: null,
      },
    },
  },

  /// Admin space configuration
  adminspace: {
    enabled: true,
    permissions: {
      read: true,
      write: false,
    },
  },
}
